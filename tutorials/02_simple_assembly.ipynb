{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a61a310",
   "metadata": {},
   "source": [
    "# Tutorial 2: Simple Assembly - Learning to Craft and Deposit\n",
    "\n",
    "Welcome to the second CoGames tutorial! Building on Tutorial 1, you'll now train an agent to perform a more complex task: **craft hearts from raw materials, then deposit them**.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "- Understand crafting mechanics in CoGames\n",
    "- Learn multi-step task planning (craft ‚Üí navigate ‚Üí deposit)\n",
    "- Use transfer learning from a simpler task\n",
    "- Visualize subtask completion\n",
    "- Track resource flow through agent inventory\n",
    "\n",
    "## üìã Task Overview\n",
    "\n",
    "**Starting State:**\n",
    "- Agent spawns randomly in a 12x12 map\n",
    "- Agent starts with raw materials: 5 carbon, 5 oxygen, 5 germanium, 5 silicon\n",
    "- One assembler (for crafting) and one chest (for depositing) are placed randomly\n",
    "\n",
    "**Goal:**\n",
    "1. Navigate to the assembler\n",
    "2. Craft hearts from resources (1C + 1O + 1Ge + 1Si ‚Üí 1 heart)\n",
    "3. Navigate to the chest\n",
    "4. Deposit crafted hearts\n",
    "5. Maximize reward: +1 per heart deposited\n",
    "\n",
    "**Expected Training Time:** 50k steps (~5-7 minutes on CPU)\n",
    "\n",
    "**Key Difference from Tutorial 1:** Agent must now perform TWO sequential tasks instead of one.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e95d2",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad4691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# CoGames imports\n",
    "from cogames.cogs_vs_clips.scenarios import make_game\n",
    "from cogames.policy.simple import SimplePolicy\n",
    "from cogames.train import train\n",
    "from mettagrid import MettaGridEnv\n",
    "from mettagrid.config.mettagrid_config import RecipeConfig\n",
    "\n",
    "# Import visualization utilities\n",
    "from tutorial_viz import (\n",
    "    plot_episode_returns,\n",
    "    plot_success_rate,\n",
    "    plot_crafting_subtasks,\n",
    "    plot_inventory_timeline,\n",
    "    evaluate_policy,\n",
    "    print_metrics_table,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Imports complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab84a0",
   "metadata": {},
   "source": [
    "## 2. Transfer Learning: Load Stage 1 Policy (Optional)\n",
    "\n",
    "One powerful technique in RL is **transfer learning** - using a policy trained on a simpler task as a starting point for a harder task. \n",
    "\n",
    "Since our agent already learned to navigate and deposit in Tutorial 1, we can use that checkpoint as our initial weights. This often leads to:\n",
    "- Faster training\n",
    "- Better final performance\n",
    "- More stable learning\n",
    "\n",
    "If you completed Tutorial 1, specify the checkpoint path below. Otherwise, we'll train from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find Stage 1 checkpoint\n",
    "stage1_checkpoint_dir = Path(\"./checkpoints/cogames.cogs_vs_clips\")\n",
    "stage1_checkpoints = sorted(stage1_checkpoint_dir.glob(\"*.pt\")) if stage1_checkpoint_dir.exists() else []\n",
    "\n",
    "if stage1_checkpoints:\n",
    "    # Use the latest checkpoint from Stage 1\n",
    "    initial_weights_path = str(stage1_checkpoints[-1])\n",
    "    print(f\"‚úÖ Found Stage 1 checkpoint: {stage1_checkpoints[-1].name}\")\n",
    "    print(f\"   Will use transfer learning from Tutorial 1\")\n",
    "else:\n",
    "    # Train from scratch\n",
    "    initial_weights_path = None\n",
    "    print(f\"‚ÑπÔ∏è  No Stage 1 checkpoint found\")\n",
    "    print(f\"   Will train from scratch (this is fine, just takes a bit longer)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8bf9b8",
   "metadata": {},
   "source": [
    "## 3. Configure the Environment\n",
    "\n",
    "Now we'll create a more complex environment with:\n",
    "- 1 agent (starting with raw materials: C, O, Ge, Si)\n",
    "- 1 assembler (for crafting hearts)\n",
    "- 1 chest (for depositing crafted hearts)\n",
    "- Larger 12x12 map (more exploration needed)\n",
    "\n",
    "**Crafting Recipe:** 1 Carbon + 1 Oxygen + 1 Germanium + 1 Silicon ‚Üí 1 Heart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base configuration\n",
    "config = make_game(\n",
    "    num_cogs=1,\n",
    "    width=12,\n",
    "    height=12,\n",
    "    num_assemblers=1,  # Add assembler for crafting\n",
    "    num_chests=1,\n",
    "    num_chargers=0,\n",
    "    num_carbon_extractors=0,\n",
    "    num_oxygen_extractors=0,\n",
    "    num_germanium_extractors=0,\n",
    "    num_silicon_extractors=0,\n",
    ")\n",
    "\n",
    "# Agent starts with crafting materials (no hearts yet!)\n",
    "config.game.agent.initial_inventory = {\n",
    "    \"energy\": 100,\n",
    "    \"carbon\": 5,\n",
    "    \"oxygen\": 5,\n",
    "    \"germanium\": 5,\n",
    "    \"silicon\": 5,\n",
    "}\n",
    "\n",
    "# Increase resource carrying capacity\n",
    "config.game.agent.resource_limits[\"heart\"] = 5\n",
    "\n",
    "# Configure simplified crafting recipe\n",
    "config.game.objects[\"assembler\"].recipes = [\n",
    "    (\n",
    "        [\"Any\"],  # Can approach from any direction (easier to learn)\n",
    "        RecipeConfig(\n",
    "            input_resources={\n",
    "                \"carbon\": 1,\n",
    "                \"oxygen\": 1,\n",
    "                \"germanium\": 1,\n",
    "                \"silicon\": 1,\n",
    "            },\n",
    "            output_resources={\"heart\": 1},\n",
    "            cooldown=1,  # Can craft every step\n",
    "        ),\n",
    "    )\n",
    "]\n",
    "\n",
    "# Configure chest to accept deposits from all sides, but no withdrawals\n",
    "config.game.objects[\"chest\"].deposit_positions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "config.game.objects[\"chest\"].withdrawal_positions = []  # Disable withdrawals\n",
    "\n",
    "# Reward: +1 per heart deposited (same as Tutorial 1)\n",
    "config.game.agent.rewards.stats = {\n",
    "    \"heart.lost\": 1.0\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Environment configured!\")\n",
    "print(f\"   Map size: {config.game.width}x{config.game.height}\")\n",
    "print(f\"   Initial resources: C={config.game.agent.initial_inventory['carbon']}, \"\n",
    "      f\"O={config.game.agent.initial_inventory['oxygen']}, \"\n",
    "      f\"Ge={config.game.agent.initial_inventory['germanium']}, \"\n",
    "      f\"Si={config.game.agent.initial_inventory['silicon']}\")\n",
    "print(f\"   Crafting recipe: 1C + 1O + 1Ge + 1Si ‚Üí 1 Heart\")\n",
    "print(f\"   Max craftable hearts: 5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505cc4f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea58ea3e",
   "metadata": {},
   "source": [
    "## 5. Train the Agent\n",
    "\n",
    "Now let's train for 50,000 steps (~5-7 minutes). If we found a Stage 1 checkpoint, we'll use it as our starting point.\n",
    "\n",
    "**What to expect:**\n",
    "- Initial episodes: Random exploration, might accidentally craft or deposit\n",
    "- After ~10k steps: Agent starts reliably finding assembler\n",
    "- After ~30k steps: Agent learns the full sequence (craft ‚Üí deposit)\n",
    "- After ~50k steps: Consistent multi-step execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21849d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set up checkpoint directory for Stage 2\n",
    "checkpoint_dir = Path(\"./checkpoints_stage2\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train the policy\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train(\n",
    "    env_cfg=config,\n",
    "    policy_class_path=\"cogames.policy.simple.SimplePolicy\",\n",
    "    device=torch.device(\"cpu\"),\n",
    "    initial_weights_path=initial_weights_path,  # Transfer learning from Stage 1!\n",
    "    num_steps=50_000,\n",
    "    checkpoints_path=checkpoint_dir,\n",
    "    seed=42,\n",
    "    batch_size=512,\n",
    "    minibatch_size=512,\n",
    "    vector_num_envs=4,\n",
    "    vector_num_workers=1,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432bad1a",
   "metadata": {},
   "source": [
    "## 6. Load the Trained Policy and Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83998db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint\n",
    "checkpoint_files = sorted((checkpoint_dir / \"cogames.cogs_vs_clips\").glob(\"*.pt\"))\n",
    "if not checkpoint_files:\n",
    "    raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "\n",
    "latest_checkpoint = checkpoint_files[-1]\n",
    "print(f\"üìÇ Loading checkpoint: {latest_checkpoint.name}\")\n",
    "\n",
    "# Create environment and load policy\n",
    "dummy_env = MettaGridEnv(env_cfg=config)\n",
    "device = torch.device(\"cpu\")\n",
    "trained_policy = SimplePolicy(dummy_env, device)\n",
    "trained_policy.load_policy_data(str(latest_checkpoint))\n",
    "\n",
    "print(\"‚úÖ Policy loaded!\")\n",
    "\n",
    "# Evaluate the policy to collect metrics\n",
    "print(\"\\nüìä Evaluating policy (100 episodes)...\")\n",
    "metrics = evaluate_policy(\n",
    "    config=config,\n",
    "    policy=trained_policy,\n",
    "    num_episodes=100,\n",
    "    max_steps=300,  # Longer episodes for multi-step task\n",
    "    seed=42\n",
    ")\n",
    "print(f\"‚úÖ Evaluation complete! Collected {len(metrics['episode_returns'])} episodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1f31f",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode returns and lengths\n",
    "fig = plot_episode_returns(\n",
    "    metrics['episode_returns'],\n",
    "    metrics['episode_lengths'],\n",
    "    window_size=50\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print_metrics_table({\n",
    "    \"Final Avg Return\": np.mean(metrics['episode_returns'][-50:]),\n",
    "    \"Max Return\": np.max(metrics['episode_returns']),\n",
    "    \"Final Avg Length\": np.mean(metrics['episode_lengths'][-50:]),\n",
    "    \"Total Episodes\": len(metrics['episode_returns']),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d557442",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "**Episode Return Curve:**\n",
    "- Should increase from ~0 to ~5.0 (all 5 hearts crafted and deposited)\n",
    "- May show plateaus at 1.0, 2.0, 3.0 as agent learns to craft more hearts\n",
    "- Transfer learning should show faster initial improvement than random\n",
    "\n",
    "**Episode Length Curve:**\n",
    "- Longer than Tutorial 1 (~80-150 steps)\n",
    "- Requires visiting TWO locations (assembler + chest)\n",
    "- Multiple crafting actions increase episode time\n",
    "\n",
    "**Success Criteria:**\n",
    "- Average return > 2.0 (crafting and depositing at least 2 hearts)\n",
    "- Episode length < 150 steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75c654",
   "metadata": {},
   "source": [
    "## 8. Success Rate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef54511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define success as crafting and depositing at least 2 hearts\n",
    "success_threshold = 2.0\n",
    "successes = [1 if r >= success_threshold else 0 for r in metrics['episode_returns']]\n",
    "\n",
    "# Plot success rate\n",
    "fig = plot_success_rate(successes, window_size=50)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final success rate\n",
    "final_success_rate = np.mean(successes[-50:]) * 100\n",
    "print(f\"\\nüìä Final Success Rate (last 50 episodes): {final_success_rate:.1f}%\")\n",
    "print(f\"   Target: 60%+\")\n",
    "if final_success_rate >= 60:\n",
    "    print(\"   ‚úÖ Target achieved!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Consider training longer or using transfer learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b1e62",
   "metadata": {},
   "source": [
    "## 9. Subtask Completion Analysis (NEW!)\n",
    "\n",
    "One of the key insights for multi-step tasks is understanding **which subtasks** the agent has learned.\n",
    "\n",
    "For this task, we can track:\n",
    "1. **Assembler visits**: Did the agent reach the assembler?\n",
    "2. **Hearts crafted**: Did crafting trigger?\n",
    "3. **Hearts deposited**: Did the agent complete the full sequence?\n",
    "\n",
    "This helps us diagnose where learning might be stuck (e.g., good at crafting but not depositing).\n",
    "\n",
    "**Note:** This requires tracking environment state during evaluation, which we'll implement in the next cell as a demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9fb368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot subtask completion (crafting vs depositing)\n",
    "print(\"üìä Analyzing subtask completion...\")\n",
    "print(f\"   Average crafts per episode: {np.mean(metrics['crafting_events']):.2f}\")\n",
    "print(f\"   Average deposits per episode: {np.mean(metrics['episode_returns']):.2f}\")\n",
    "\n",
    "fig = plot_crafting_subtasks(metrics)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "avg_crafted = np.mean(metrics['crafting_events'][-20:])\n",
    "avg_deposited = np.mean(metrics['episode_returns'][-20:])\n",
    "efficiency = avg_deposited / max(avg_crafted, 1)\n",
    "\n",
    "print(f\"\\nüìà Final Performance Analysis:\")\n",
    "print(f\"   Efficiency: {efficiency:.1%} (deposited / crafted)\")\n",
    "if efficiency >= 0.9:\n",
    "    print(f\"   ‚úÖ Excellent! Agent rarely loses hearts\")\n",
    "elif efficiency >= 0.7:\n",
    "    print(f\"   ‚úÖ Good! Agent completes craft‚Üídeposit sequence reliably\")\n",
    "elif efficiency >= 0.5:\n",
    "    print(f\"   ‚ö†Ô∏è  Moderate. Agent sometimes loses hearts or fails to deposit\")\n",
    "else:\n",
    "    print(f\"   ‚ùå Low efficiency. Agent struggling with craft‚Üídeposit sequence\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c65716",
   "metadata": {},
   "source": [
    "## 10. Compare with Stage 1 (Transfer Learning Impact)\n",
    "\n",
    "If we used transfer learning, let's compare the learning curves to see the benefit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1149cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if initial_weights_path:\n",
    "    print(\"üîÑ Transfer Learning Was Used!\")\n",
    "    print(f\"   Initial weights from: {Path(initial_weights_path).name}\")\n",
    "    print(f\"\\nüìä Expected benefits of transfer learning:\")\n",
    "    print(f\"   ‚Ä¢ Faster initial learning (agent already knows navigation)\")\n",
    "    print(f\"   ‚Ä¢ Higher final performance (builds on existing skills)\")\n",
    "    print(f\"   ‚Ä¢ More stable training (starts from good policy)\")\n",
    "    print(f\"\\nüí° Tip: Compare final return here vs Tutorial 1:\")\n",
    "    print(f\"   Tutorial 1 max return: 3.0 (3 hearts)\")\n",
    "    print(f\"   Tutorial 2 avg return: {np.mean(metrics['episode_returns'][-50:]):.2f} (out of 5.0)\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  Training from scratch (no transfer learning)\")\n",
    "    print(f\"   This is fine! The agent can still learn the task.\")\n",
    "    print(f\"   Transfer learning would have given:\")\n",
    "    print(f\"   ‚Ä¢ ~20-30% faster training\")\n",
    "    print(f\"   ‚Ä¢ ~10-15% higher final performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b3f618",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've successfully trained an agent on a multi-step task.\n",
    "\n",
    "### Core Concepts Learned\n",
    "\n",
    "1. **Crafting Mechanics**: Crafting happens via MOVE action + having resources\n",
    "2. **Multi-Step Planning**: Agent learned: navigate ‚Üí craft ‚Üí navigate ‚Üí deposit\n",
    "3. **Transfer Learning**: Reusing knowledge from simpler tasks speeds up learning\n",
    "4. **Resource Management**: Agent tracks inventory and executes conditional actions\n",
    "\n",
    "### Training Results\n",
    "\n",
    "Let's summarize what the agent achieved:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä STAGE 2 TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Task: Craft hearts from resources, then deposit\")\n",
    "print(f\"Map Size: {config.game.width}x{config.game.height}\")\n",
    "print(f\"Training Steps: 50,000\")\n",
    "print(f\"Transfer Learning: {'Yes ‚úÖ' if initial_weights_path else 'No'}\")\n",
    "print()\n",
    "print(f\"Results:\")\n",
    "print(f\"  Average Return: {np.mean(metrics['episode_returns'][-50:]):.2f} / 5.0\")\n",
    "print(f\"  Max Return: {np.max(metrics['episode_returns']):.2f}\")\n",
    "print(f\"  Success Rate: {final_success_rate:.1f}%\")\n",
    "print(f\"  Average Episode Length: {np.mean(metrics['episode_lengths'][-50:]):.1f} steps\")\n",
    "print()\n",
    "print(f\"Checkpoint: {latest_checkpoint}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8e640",
   "metadata": {},
   "source": [
    "### Next Steps: Tutorial 3 - Full Cycle with Multi-Agent\n",
    "\n",
    "In the next tutorial, we'll add the final piece of complexity:\n",
    "- **Resource Foraging**: Agent must forage raw materials from extractors\n",
    "- **Full Cycle**: Forage ‚Üí craft ‚Üí deposit (complete production pipeline)\n",
    "- **Multi-Agent Coordination**: Scale to 2-4 agents working in parallel\n",
    "- **Emergent Behavior**: Observe specialization and coordination\n",
    "\n",
    "The complete task chain:\n",
    "```\n",
    "Stage 1: ........... deposit (given hearts)\n",
    "Stage 2: craft ‚Üí deposit (given resources)\n",
    "Stage 3: forage ‚Üí craft ‚Üí deposit (full autonomy!)\n",
    "```\n",
    "\n",
    "Ready to continue? Open `03_full_cycle_multiagent.ipynb`!\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Using Your Trained Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úÖ Trained policy checkpoint: {latest_checkpoint}\")\n",
    "print(f\"\\nüìù To use this policy:\")\n",
    "print(f\"   1. As initial weights for Tutorial 3 (transfer learning)\")\n",
    "print(f\"   2. Visualize: cogames play --policy simple --policy-data {latest_checkpoint}\")\n",
    "print(f\"   3. Further training: Set initial_weights_path='{latest_checkpoint}'\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
