{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoGames Puffer Training - Scrambler Tutorial\n",
    "\n",
    "This notebook trains a scrambler agent from scratch.\n",
    "\n",
    "It walks through:\n",
    "1. Building a scrambler-focused environment\n",
    "2. Token-to-grid observation preprocessing\n",
    "3. Defining a CNN + LSTM policy network from scratch\n",
    "4. Vectorizing with PufferLib\n",
    "5. Running the PuffeRL training loop\n",
    "6. Uploading to the CoGames leaderboard\n",
    "\n",
    "**Scrambler role**: Disalign (scramble) enemy-controlled junctions.\n",
    "Scramblers have massively increased HP (+200), making them durable enough\n",
    "to survive in enemy territory. They earn large rewards for scrambling\n",
    "junctions, denying territory control to opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mettagrid cogames pufferlib-core --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "import pufferlib.vector as pvector\n",
    "from pufferlib import pufferl\n",
    "from pufferlib.pufferlib import set_buffers\n",
    "\n",
    "from mettagrid import MettaGridConfig\n",
    "from mettagrid.envs.mettagrid_puffer_env import MettaGridPufferEnv\n",
    "from mettagrid.envs.early_reset_handler import EarlyResetHandler\n",
    "from mettagrid.envs.stats_tracker import StatsTracker\n",
    "from mettagrid.mapgen.mapgen import MapGen\n",
    "from mettagrid.policy.policy_env_interface import PolicyEnvInterface\n",
    "from mettagrid.simulator import Simulator\n",
    "from mettagrid.util.stats_writer import NoopStatsWriter\n",
    "\n",
    "from cogames.cogs_vs_clips.clip_difficulty import EASY\n",
    "from cogames.cogs_vs_clips.mission import CvCMission\n",
    "from cogames.cogs_vs_clips.scrambler_tutorial import ScramblerRewardsVariant\n",
    "from cogames.cogs_vs_clips.sites import COGSGUARD_ARENA\n",
    "from cogames.cogs_vs_clips.team import CogTeam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the mission and environment config\n",
    "\n",
    "- **Site**: CogsGuard Arena (50x50 compact training map)\n",
    "- **EASY difficulty**: No clips pressure\n",
    "- **initial_hearts=120**: Hearts available for scrambling junctions\n",
    "- **1000 max steps** per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 4\n",
    "MAX_STEPS = 1000\n",
    "\n",
    "mission = CvCMission(\n",
    "    name=\"scrambler_tutorial\",\n",
    "    description=\"Learn scrambler role - scramble enemy junctions.\",\n",
    "    site=COGSGUARD_ARENA,\n",
    "    num_cogs=NUM_AGENTS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    teams={\"cogs\": CogTeam(name=\"cogs\", num_agents=NUM_AGENTS, wealth=3, initial_hearts=120)},\n",
    "    variants=[EASY, ScramblerRewardsVariant()],\n",
    ")\n",
    "\n",
    "env_cfg: MettaGridConfig = mission.make_env()\n",
    "\n",
    "print(f\"Map builder: {type(env_cfg.game.map_builder).__name__}\")\n",
    "print(f\"Max steps: {env_cfg.game.max_steps}\")\n",
    "print(f\"Num agents: {env_cfg.game.num_agents}\")\n",
    "print(f\"Events: {list(env_cfg.game.events.keys())}\")\n",
    "print(f\"Collectives: {list(env_cfg.game.collectives.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a single environment\n",
    "\n",
    "MettaGridPufferEnv wraps the C++ simulator with PufferLib's PufferEnv interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "def make_env(buf=None, seed=None):\n",
    "    \"\"\"Environment factory for PufferLib vectorization.\"\"\"\n",
    "    cfg = env_cfg.model_copy(deep=True)\n",
    "\n",
    "    map_builder = cfg.game.map_builder\n",
    "    if isinstance(map_builder, MapGen.Config) and seed is not None:\n",
    "        map_builder.seed = SEED + seed\n",
    "\n",
    "    simulator = Simulator()\n",
    "    simulator.add_event_handler(StatsTracker(NoopStatsWriter()))\n",
    "    simulator.add_event_handler(EarlyResetHandler())\n",
    "    env = MettaGridPufferEnv(simulator, cfg, buf=buf, seed=seed or 0)\n",
    "    set_buffers(env, buf)\n",
    "    return env\n",
    "\n",
    "\n",
    "driver_env = make_env(seed=0)\n",
    "policy_env_info = PolicyEnvInterface.from_mg_cfg(driver_env.env_cfg)\n",
    "\n",
    "print(f\"Observation space: {driver_env.single_observation_space}\")\n",
    "print(f\"Action space: {driver_env.single_action_space}\")\n",
    "print(f\"Num agents: {driver_env.num_agents}\")\n",
    "print(f\"Action names: {policy_env_info.action_names}\")\n",
    "print(f\"Obs features: {len(policy_env_info.obs_features)} features\")\n",
    "print(f\"Obs grid: {policy_env_info.obs_height}x{policy_env_info.obs_width}\")\n",
    "driver_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Observation preprocessing\n",
    "\n",
    "MettaGrid observations are sparse tokens `[B, T, 3]` where each token is `[packed_xy, feature_id, value]`.\n",
    "The packed byte encodes grid coordinates as nibbles: `y = byte >> 4, x = byte & 0x0F`.\n",
    "\n",
    "We scatter these into a dense spatial grid `[B, C, H, W]` so a CNN can process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_grid(\n",
    "    observations: torch.Tensor,\n",
    "    obs_height: int,\n",
    "    obs_width: int,\n",
    "    num_features: int,\n",
    "    feature_scale: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Convert sparse token observations [B, T, 3] into a dense grid [B, C, H, W].\"\"\"\n",
    "    batch_size = observations.shape[0]\n",
    "    device = observations.device\n",
    "\n",
    "    coords_byte = observations[..., 0].to(torch.long)\n",
    "    x_coords = coords_byte & 0x0F\n",
    "    y_coords = (coords_byte >> 4) & 0x0F\n",
    "    feature_ids = observations[..., 1].to(torch.long)\n",
    "    values = observations[..., 2].to(torch.float32)\n",
    "\n",
    "    valid_mask = (observations[..., 0] != 0xFF).float()\n",
    "    x_coords = torch.clamp(x_coords, 0, obs_width - 1)\n",
    "    y_coords = torch.clamp(y_coords, 0, obs_height - 1)\n",
    "    feature_ids_clamped = torch.clamp(feature_ids, 0, num_features - 1)\n",
    "\n",
    "    scale = feature_scale[torch.clamp(feature_ids, 0, feature_scale.shape[0] - 1)]\n",
    "    values = (values / (scale + 1e-6)) * valid_mask\n",
    "\n",
    "    grid = torch.zeros(batch_size, num_features, obs_height, obs_width, device=device)\n",
    "    batch_idx = torch.arange(batch_size, device=device).unsqueeze(1).expand_as(x_coords)\n",
    "    linear_idx = (\n",
    "        batch_idx * (num_features * obs_height * obs_width)\n",
    "        + feature_ids_clamped * (obs_height * obs_width)\n",
    "        + y_coords * obs_width\n",
    "        + x_coords\n",
    "    )\n",
    "    grid.view(-1).scatter_add_(0, linear_idx.view(-1), values.view(-1))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the neural network\n",
    "\n",
    "CNN + LSTM actor-critic:\n",
    "- **CNN encoder**: Two 3x3 conv layers (64 \u2192 128) with stride 2, projected to 256-dim\n",
    "- **Self encoder**: Linear on the center cell (agent's own state) \u2192 256-dim\n",
    "- **LSTM**: 512 hidden units, 1 layer\n",
    "- **Action head**: Linear \u2192 num_actions logits\n",
    "- **Value head**: Linear \u2192 scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "\n",
    "class ScramblerPolicyNet(nn.Module):\n",
    "    \"\"\"CNN + LSTM actor-critic.\"\"\"\n",
    "\n",
    "    _feature_scale: torch.Tensor\n",
    "\n",
    "    def __init__(self, env_info: PolicyEnvInterface):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = 512\n",
    "        self._obs_height = env_info.obs_height\n",
    "        self._obs_width = env_info.obs_width\n",
    "        self._num_features = max((int(f.id) for f in env_info.obs_features), default=0) + 1\n",
    "\n",
    "        feature_norms = {f.id: f.normalization for f in env_info.obs_features}\n",
    "        max_id = max((int(fid) for fid in feature_norms.keys()), default=-1)\n",
    "        feature_scale = torch.ones(max(256, max_id + 1), dtype=torch.float32)\n",
    "        for fid, norm in feature_norms.items():\n",
    "            feature_scale[fid] = max(float(norm), 1.0)\n",
    "        self.register_buffer(\"_feature_scale\", feature_scale)\n",
    "\n",
    "        self._cnn = nn.Sequential(\n",
    "            nn.Conv2d(self._num_features, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, self._num_features, self._obs_height, self._obs_width)\n",
    "            cnn_out_size = self._cnn(dummy).shape[1]\n",
    "\n",
    "        self._cnn_fc = nn.Linear(cnn_out_size, 256)\n",
    "        self._self_encoder = nn.Linear(self._num_features, 256)\n",
    "        self._rnn = nn.LSTM(self.hidden_size, self.hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "        num_actions = len(env_info.action_names)\n",
    "        self._action_head = nn.Linear(self.hidden_size, num_actions)\n",
    "        self._value_head = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, observations: torch.Tensor, state: dict[str, torch.Tensor] | None = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        orig_shape = observations.shape\n",
    "        if observations.dim() == 4:\n",
    "            segments, bptt_horizon = orig_shape[0], orig_shape[1]\n",
    "            observations = observations.reshape(segments * bptt_horizon, *orig_shape[2:])\n",
    "        else:\n",
    "            segments, bptt_horizon = orig_shape[0], 1\n",
    "\n",
    "        grid = tokens_to_grid(observations, self._obs_height, self._obs_width, self._num_features, self._feature_scale)\n",
    "        cnn_out = torch.relu(self._cnn_fc(self._cnn(grid)))\n",
    "        center = grid[:, :, self._obs_height // 2, self._obs_width // 2]\n",
    "        self_out = torch.relu(self._self_encoder(center))\n",
    "\n",
    "        hidden = torch.cat([cnn_out, self_out], dim=-1)\n",
    "        hidden = rearrange(hidden, \"(b t) h -> b t h\", t=bptt_horizon, b=segments)\n",
    "\n",
    "        rnn_state = None\n",
    "        if state is not None:\n",
    "            h, c = state.get(\"lstm_h\"), state.get(\"lstm_c\")\n",
    "            if h is not None and c is not None:\n",
    "                h = h.transpose(0, 1) if h.dim() == 3 else h.unsqueeze(0)\n",
    "                c = c.transpose(0, 1) if c.dim() == 3 else c.unsqueeze(0)\n",
    "                rnn_state = (h, c)\n",
    "\n",
    "        hidden, (h_out, c_out) = self._rnn(hidden, rnn_state)\n",
    "\n",
    "        if state is not None and \"lstm_h\" in state:\n",
    "            state[\"lstm_h\"] = h_out.transpose(0, 1)\n",
    "            state[\"lstm_c\"] = c_out.transpose(0, 1)\n",
    "\n",
    "        hidden = rearrange(hidden, \"b t h -> (b t) h\")\n",
    "        return self._action_head(hidden), self._value_head(hidden)\n",
    "\n",
    "    forward_eval = forward\n",
    "\n",
    "\n",
    "net = ScramblerPolicyNet(policy_env_info).to(DEVICE)\n",
    "\n",
    "print(f\"\\nArchitecture:\\n{net}\")\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorize environments with PufferLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENVS = 4\n",
    "\n",
    "vecenv = pvector.make(\n",
    "    make_env,\n",
    "    num_envs=NUM_ENVS,\n",
    "    num_workers=1,\n",
    "    batch_size=NUM_ENVS,\n",
    "    backend=pvector.Serial,\n",
    ")\n",
    "\n",
    "total_agents = vecenv.num_agents\n",
    "print(f\"Vectorized envs: {NUM_ENVS}\")\n",
    "print(f\"Total agents across all envs: {total_agents}\")\n",
    "print(f\"Agents per env: {total_agents // NUM_ENVS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure and run PuffeRL training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TIMESTEPS = 100_000_000\n",
    "BPTT_HORIZON = 64\n",
    "BATCH_SIZE = max(4096, total_agents * BPTT_HORIZON)\n",
    "MINIBATCH_SIZE = min(4096, BATCH_SIZE)\n",
    "\n",
    "train_config = dict(\n",
    "    env=\"cogames.cogs_vs_clips\",\n",
    "    device=DEVICE.type,\n",
    "    total_timesteps=max(TOTAL_TIMESTEPS, BATCH_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    minibatch_size=MINIBATCH_SIZE,\n",
    "    bptt_horizon=BPTT_HORIZON,\n",
    "    seed=SEED,\n",
    "    use_rnn=True,\n",
    "    torch_deterministic=True,\n",
    "    cpu_offload=False,\n",
    "    compile=False,\n",
    "    optimizer=\"adam\",\n",
    "    learning_rate=0.00092,\n",
    "    anneal_lr=True,\n",
    "    min_lr_ratio=0.0,\n",
    "    adam_beta1=0.95,\n",
    "    adam_beta2=0.999,\n",
    "    adam_eps=1e-8,\n",
    "    precision=\"float32\",\n",
    "    gamma=0.995,\n",
    "    gae_lambda=0.90,\n",
    "    update_epochs=1,\n",
    "    clip_coef=0.2,\n",
    "    vf_coef=2.0,\n",
    "    vf_clip_coef=0.2,\n",
    "    max_grad_norm=1.5,\n",
    "    ent_coef=0.01,\n",
    "    vtrace_rho_clip=1.0,\n",
    "    vtrace_c_clip=1.0,\n",
    "    prio_alpha=0.8,\n",
    "    prio_beta0=0.2,\n",
    "    data_dir=\"./train_dir\",\n",
    "    checkpoint_interval=50,\n",
    "    max_minibatch_size=32768,\n",
    ")\n",
    "\n",
    "print(\"Training config:\")\n",
    "for k, v in train_config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pufferl.PuffeRL(train_config, vecenv, net)\n",
    "print(f\"Model size: {trainer.model_size:,} params\")\n",
    "print(f\"Batch size: {trainer.config['batch_size']}\")\n",
    "print(f\"Total epochs: {trainer.total_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "while trainer.global_step < train_config[\"total_timesteps\"]:\n",
    "    trainer.evaluate()\n",
    "    trainer.train()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    trainer.print_dashboard()\n",
    "\n",
    "    has_nan = any(\n",
    "        (p.grad is not None and not p.grad.isfinite().all()) or not p.isfinite().all()\n",
    "        for p in net.parameters()\n",
    "    )\n",
    "    if has_nan:\n",
    "        print(f\"Training diverged at step {trainer.global_step}!\")\n",
    "        break\n",
    "\n",
    "trainer.close()\n",
    "print(f\"Training complete. Steps: {trainer.global_step}, Epochs: {trainer.epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./train_dir/tutorial_scrambler.pt\"\n",
    "torch.save(net.state_dict(), save_path)\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Watch the trained policy\n",
    "\n",
    "Run the trained network in a fresh environment and render each step as a Unicode grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "from mettagrid.renderer.miniscope.buffer import MapBuffer\n",
    "from mettagrid.renderer.miniscope.symbol import DEFAULT_SYMBOL_MAP\n",
    "\n",
    "VIEWPORT_HEIGHT = 25\n",
    "VIEWPORT_WIDTH = 35\n",
    "\n",
    "render_env = make_env(seed=99)\n",
    "obs, _ = render_env.reset()\n",
    "\n",
    "# Build symbol map (same as render_env.render() does internally)\n",
    "symbol_map = DEFAULT_SYMBOL_MAP.copy()\n",
    "for obj in render_env._current_cfg.game.objects.values():\n",
    "    if obj.render_name:\n",
    "        symbol_map[obj.render_name] = obj.render_symbol\n",
    "    symbol_map[obj.name] = obj.render_symbol\n",
    "\n",
    "sim = render_env._sim\n",
    "\n",
    "# Center viewport on hub (fall back to map center)\n",
    "grid_objects = sim._c_sim.grid_objects()\n",
    "hub_r, hub_c = sim.map_height // 2, sim.map_width // 2\n",
    "for obj in grid_objects.values():\n",
    "    if \"hub\" in obj[\"type_name\"]:\n",
    "        hub_r, hub_c = obj[\"r\"], obj[\"c\"]\n",
    "        break\n",
    "\n",
    "num_agents = render_env.num_agents\n",
    "state = {\n",
    "    \"lstm_h\": torch.zeros(num_agents, 1, net.hidden_size, device=DEVICE),\n",
    "    \"lstm_c\": torch.zeros(num_agents, 1, net.hidden_size, device=DEVICE),\n",
    "}\n",
    "\n",
    "net.eval()\n",
    "for step in range(200):\n",
    "    obs_tensor = torch.from_numpy(obs).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = net(obs_tensor, state)\n",
    "    actions = torch.distributions.Categorical(logits=logits).sample().cpu().numpy()\n",
    "    obs, rewards, terms, truncs, infos = render_env.step(actions)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    buf = MapBuffer(\n",
    "        symbol_map=symbol_map,\n",
    "        initial_height=sim.map_height,\n",
    "        initial_width=sim.map_width,\n",
    "    )\n",
    "    buf.set_viewport(hub_r, hub_c, VIEWPORT_HEIGHT, VIEWPORT_WIDTH)\n",
    "    rendered = buf.render(sim._c_sim.grid_objects())\n",
    "    print(f\"Step {step}\")\n",
    "    print(rendered)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "render_env.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload to the leaderboard\n",
    "\n",
    "Submit the trained weights to the CoGames tournament. This uses the `tutorial` policy class\n",
    "(same architecture as `ScramblerPolicyNet`) with our saved weights.\n",
    "\n",
    "Prerequisites: run `cogames login` in a terminal first to authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_NAME = \"my-scrambler\"  # Change this to your desired policy name\n",
    "\n",
    "!cogames upload -p \"class=tutorial,data={save_path}\" -n {POLICY_NAME} --skip-validation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}