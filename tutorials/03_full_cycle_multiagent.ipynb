{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf9d0801",
   "metadata": {},
   "source": [
    "# Tutorial 3: Full Cycle + Multi-Agent - Complete Autonomous Production\n",
    "\n",
    "Welcome to the final CoGames tutorial! You'll now train agents to perform the **complete production cycle**: forage resources, craft hearts, and deposit them. Then, you'll scale to **multiple agents** working in parallel.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- Understand resource foraging mechanics\n",
    "- Master the complete 3-step task chain (forage â†’ craft â†’ deposit)\n",
    "- Scale from single to multi-agent training\n",
    "- Observe emergent coordination behaviors\n",
    "- Analyze multi-agent performance metrics\n",
    "\n",
    "## ðŸ“‹ Task Overview\n",
    "\n",
    "### Part A: Single-Agent Foraging (100k steps)\n",
    "\n",
    "**Starting State:**\n",
    "- Agent spawns in a 15x15 map\n",
    "- Agent starts with oxygen, germanium, silicon BUT NO CARBON\n",
    "- Must forage carbon from extractors\n",
    "- 3 carbon extractors, 1 assembler, 1 chest placed randomly\n",
    "\n",
    "**Goal:**\n",
    "1. Navigate to carbon extractor\n",
    "2. Forage carbon (by moving into extractor)\n",
    "3. Navigate to assembler\n",
    "4. Craft hearts (1C + 1O + 1Ge + 1Si â†’ 1 heart)\n",
    "5. Navigate to chest\n",
    "6. Deposit hearts\n",
    "7. Repeat!\n",
    "\n",
    "### Part B: Multi-Agent Coordination (200k steps)\n",
    "\n",
    "**Starting State:**\n",
    "- **4 agents** spawn in the same 15x15 map\n",
    "- All agents share resources and goals\n",
    "- Same objects as Part A\n",
    "\n",
    "**Goal:**\n",
    "- Agents must coordinate implicitly\n",
    "- Share extractors efficiently\n",
    "- Avoid collisions\n",
    "- Maximize team reward\n",
    "\n",
    "**Expected Training Time:** \n",
    "- Part A: ~10-15 minutes\n",
    "- Part B: ~15-20 minutes\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf75ba",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# CoGames imports\n",
    "from cogames.cogs_vs_clips.scenarios import make_game\n",
    "from cogames.policy.simple import SimplePolicy\n",
    "from cogames.train import train\n",
    "from mettagrid import MettaGridEnv\n",
    "from mettagrid.config.mettagrid_config import RecipeConfig\n",
    "\n",
    "# Import visualization utilities\n",
    "from tutorial_viz import (\n",
    "    plot_episode_returns,\n",
    "    plot_success_rate,\n",
    "    plot_crafting_subtasks,\n",
    "    plot_multiagent_returns,\n",
    "    plot_coordination_metrics,\n",
    "    evaluate_policy,\n",
    "    print_metrics_table,\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Imports complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a594d06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find Stage 2 checkpoint\n",
    "stage2_checkpoint_dir = Path(\"./checkpoints_stage2/cogames.cogs_vs_clips\")\n",
    "stage2_checkpoints = sorted(stage2_checkpoint_dir.glob(\"*.pt\")) if stage2_checkpoint_dir.exists() else []\n",
    "\n",
    "if stage2_checkpoints:\n",
    "    initial_weights_path = str(stage2_checkpoints[-1])\n",
    "    print(f\"âœ… Found Stage 2 checkpoint: {stage2_checkpoints[-1].name}\")\n",
    "    print(f\"   Will use transfer learning from Tutorial 2\")\n",
    "else:\n",
    "    initial_weights_path = None\n",
    "    print(f\"â„¹ï¸  No Stage 2 checkpoint found\")\n",
    "    print(f\"   Will train from scratch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650f23b",
   "metadata": {},
   "source": [
    "## Part A: Single-Agent Foraging\n",
    "\n",
    "### 3. Configure Foraging Environment\n",
    "\n",
    "Now we'll add the final complexity: **resource extraction**. The agent must forage carbon before it can craft hearts.\n",
    "\n",
    "**Key additions:**\n",
    "- 3 carbon extractors (distributed around map)\n",
    "- Agent starts WITHOUT carbon\n",
    "- Must forage â†’ craft â†’ deposit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749a62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create foraging configuration (single agent)\n",
    "config_single = make_game(\n",
    "    num_cogs=1,\n",
    "    width=15,\n",
    "    height=15,\n",
    "    num_assemblers=1,\n",
    "    num_chests=1,\n",
    "    num_chargers=0,\n",
    "    num_carbon_extractors=3,  # ADD: Carbon extractors\n",
    "    num_oxygen_extractors=0,\n",
    "    num_germanium_extractors=0,\n",
    "    num_silicon_extractors=0,\n",
    ")\n",
    "\n",
    "# Agent starts with 3 resources but MUST forage carbon\n",
    "config_single.game.agent.initial_inventory = {\n",
    "    \"energy\": 100,\n",
    "    \"oxygen\": 5,\n",
    "    \"germanium\": 5,\n",
    "    \"silicon\": 5,\n",
    "    # NO carbon! Must forage it\n",
    "}\n",
    "\n",
    "# Configure carbon extractors\n",
    "config_single.game.objects[\"carbon_extractor\"].recipes[0][1].output_resources = {\n",
    "    \"carbon\": 1  # Give 1 carbon per extraction\n",
    "}\n",
    "\n",
    "# Same crafting recipe as Stage 2\n",
    "config_single.game.objects[\"assembler\"].recipes = [\n",
    "    (\n",
    "        [\"Any\"],\n",
    "        RecipeConfig(\n",
    "            input_resources={\n",
    "                \"carbon\": 1,\n",
    "                \"oxygen\": 1,\n",
    "                \"germanium\": 1,\n",
    "                \"silicon\": 1,\n",
    "            },\n",
    "            output_resources={\"heart\": 1},\n",
    "            cooldown=1,\n",
    "        ),\n",
    "    )\n",
    "]\n",
    "\n",
    "# Configure chest\n",
    "config_single.game.objects[\"chest\"].deposit_positions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "config_single.game.objects[\"chest\"].withdrawal_positions = []\n",
    "\n",
    "# Same reward\n",
    "config_single.game.agent.rewards.stats = {\n",
    "    \"heart.lost\": 1.0\n",
    "}\n",
    "\n",
    "print(\"âœ… Single-agent foraging environment configured!\")\n",
    "print(f\"   Map size: {config_single.game.width}x{config_single.game.height}\")\n",
    "print(f\"   Carbon extractors: 3\")\n",
    "print(f\"   Initial carbon: 0 (must forage)\")\n",
    "print(f\"   Task: forage â†’ craft â†’ deposit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02253979",
   "metadata": {},
   "source": [
    "### 4. Understanding Foraging Mechanics\n",
    "\n",
    "**How does foraging work?**\n",
    "\n",
    "Similar to crafting, there's no explicit \"FORAGE\" action. Instead:\n",
    "1. Agent moves **into** an extractor from any position\n",
    "2. If extractor has resources available, resources are added to agent's inventory\n",
    "3. Extractor may have cooldown or limited resources\n",
    "\n",
    "**Example Flow:**\n",
    "```\n",
    "1. Agent at (5, 5), Carbon Extractor at (6, 5)\n",
    "2. Agent has: {carbon: 0, oxygen: 5, germanium: 5, silicon: 5}\n",
    "3. Agent action: MOVE EAST â†’ Agent moves to (6, 5)\n",
    "4. Environment detects: Agent on extractor\n",
    "5. Extraction triggers: Carbon added to inventory\n",
    "6. Agent now has: {carbon: 1, oxygen: 5, ...}\n",
    "```\n",
    "\n",
    "**Complete Cycle:**\n",
    "1. Forage carbon from extractor (5 times)\n",
    "2. Navigate to assembler\n",
    "3. Craft heart (consumes 1C+1O+1Ge+1Si)\n",
    "4. Navigate to chest\n",
    "5. Deposit heart (+1 reward)\n",
    "6. Repeat until resources depleted!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af659e",
   "metadata": {},
   "source": [
    "### 5. Train Single-Agent on Full Cycle\n",
    "\n",
    "Training for 100k steps to learn the complete foraging â†’ crafting â†’ depositing sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33684281",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set up checkpoint directory\n",
    "checkpoint_dir_single = Path(\"./checkpoints_stage3_single\")\n",
    "checkpoint_dir_single.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ðŸš€ Starting single-agent training (100k steps)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train(\n",
    "    env_cfg=config_single,\n",
    "    policy_class_path=\"cogames.policy.simple.SimplePolicy\",\n",
    "    device=torch.device(\"cpu\"),\n",
    "    initial_weights_path=initial_weights_path,  # Transfer from Stage 2\n",
    "    num_steps=100_000,\n",
    "    checkpoints_path=checkpoint_dir_single,\n",
    "    seed=42,\n",
    "    batch_size=512,\n",
    "    minibatch_size=512,\n",
    "    vector_num_envs=4,\n",
    "    vector_num_workers=1,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Single-agent training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb21053",
   "metadata": {},
   "source": [
    "### 6. Evaluate Single-Agent Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbd12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_files_single = sorted((checkpoint_dir_single / \"cogames.cogs_vs_clips\").glob(\"*.pt\"))\n",
    "latest_checkpoint_single = checkpoint_files_single[-1]\n",
    "print(f\"ðŸ“‚ Loading checkpoint: {latest_checkpoint_single.name}\")\n",
    "\n",
    "dummy_env_single = MettaGridEnv(env_cfg=config_single)\n",
    "device = torch.device(\"cpu\")\n",
    "trained_policy_single = SimplePolicy(dummy_env_single, device)\n",
    "trained_policy_single.load_policy_data(str(latest_checkpoint_single))\n",
    "\n",
    "print(\"âœ… Policy loaded!\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nðŸ“Š Evaluating single-agent policy...\")\n",
    "metrics_single = evaluate_policy(\n",
    "    config=config_single,\n",
    "    policy=trained_policy_single,\n",
    "    num_episodes=100,\n",
    "    max_steps=400,  # Longer episodes for full cycle\n",
    "    seed=42\n",
    ")\n",
    "print(f\"âœ… Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823a8e9",
   "metadata": {},
   "source": [
    "### 7. Visualize Single-Agent Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3929a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig = plot_episode_returns(\n",
    "    metrics_single['episode_returns'],\n",
    "    metrics_single['episode_lengths'],\n",
    "    window_size=50\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print stats\n",
    "print_metrics_table({\n",
    "    \"Avg Return (last 50)\": np.mean(metrics_single['episode_returns'][-50:]),\n",
    "    \"Max Return\": np.max(metrics_single['episode_returns']),\n",
    "    \"Avg Length (last 50)\": np.mean(metrics_single['episode_lengths'][-50:]),\n",
    "    \"Success Rate (â‰¥1 heart)\": np.mean([1 if r >= 1 else 0 for r in metrics_single['episode_returns'][-50:]]) * 100,\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š Task Complexity:\")\n",
    "print(\"   Tutorial 1: deposit (1 step)\")\n",
    "print(\"   Tutorial 2: craft â†’ deposit (2 steps)\")\n",
    "print(f\"   Tutorial 3: forage â†’ craft â†’ deposit (3 steps)\")\n",
    "print(f\"   Expected return: 1-5 hearts (harder than previous tutorials)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80484af",
   "metadata": {},
   "source": [
    "## Part B: Multi-Agent Coordination\n",
    "\n",
    "### 8. Scale to 4 Agents\n",
    "\n",
    "Now for the exciting part! We'll train **4 agents** simultaneously in the same environment. \n",
    "\n",
    "**Key Changes:**\n",
    "- `num_cogs=4` instead of 1\n",
    "- All agents share the same environment\n",
    "- Implicit coordination emerges from training\n",
    "- Team reward (all agents get +1 when ANY agent deposits)\n",
    "\n",
    "**Expected Behaviors:**\n",
    "- Agents learn to share extractors\n",
    "- Avoid collisions\n",
    "- Potential role specialization\n",
    "- Emergent coordination without explicit communication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-agent configuration\n",
    "config_multi = make_game(\n",
    "    num_cogs=4,  # 4 agents!\n",
    "    width=15,\n",
    "    height=15,\n",
    "    num_assemblers=1,\n",
    "    num_chests=1,\n",
    "    num_chargers=0,\n",
    "    num_carbon_extractors=3,\n",
    "    num_oxygen_extractors=0,\n",
    "    num_germanium_extractors=0,\n",
    "    num_silicon_extractors=0,\n",
    ")\n",
    "\n",
    "# Same starting inventory\n",
    "config_multi.game.agent.initial_inventory = {\n",
    "    \"energy\": 100,\n",
    "    \"oxygen\": 5,\n",
    "    \"germanium\": 5,\n",
    "    \"silicon\": 5,\n",
    "}\n",
    "\n",
    "# Same extractor config\n",
    "config_multi.game.objects[\"carbon_extractor\"].recipes[0][1].output_resources = {\n",
    "    \"carbon\": 1\n",
    "}\n",
    "\n",
    "# Same crafting recipe\n",
    "config_multi.game.objects[\"assembler\"].recipes = [\n",
    "    (\n",
    "        [\"Any\"],\n",
    "        RecipeConfig(\n",
    "            input_resources={\n",
    "                \"carbon\": 1,\n",
    "                \"oxygen\": 1,\n",
    "                \"germanium\": 1,\n",
    "                \"silicon\": 1,\n",
    "            },\n",
    "            output_resources={\"heart\": 1},\n",
    "            cooldown=1,\n",
    "        ),\n",
    "    )\n",
    "]\n",
    "\n",
    "# Same chest config\n",
    "config_multi.game.objects[\"chest\"].deposit_positions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "config_multi.game.objects[\"chest\"].withdrawal_positions = []\n",
    "\n",
    "# Same reward (but now 4 agents share it)\n",
    "config_multi.game.agent.rewards.stats = {\n",
    "    \"heart.lost\": 1.0\n",
    "}\n",
    "\n",
    "print(\"âœ… Multi-agent environment configured!\")\n",
    "print(f\"   Agents: 4\")\n",
    "print(f\"   Map size: {config_multi.game.width}x{config_multi.game.height}\")\n",
    "print(f\"   Coordination: Implicit (emerges from training)\")\n",
    "print(f\"   Reward: Per-agent (+1 when THAT agent deposits)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b153b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ec81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set up checkpoint directory\n",
    "checkpoint_dir_multi = Path(\"./checkpoints_stage3_multi\")\n",
    "checkpoint_dir_multi.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use single-agent checkpoint as starting point!\n",
    "initial_weights_multi = str(latest_checkpoint_single) if latest_checkpoint_single else None\n",
    "\n",
    "print(\"ðŸš€ Starting multi-agent training (200k steps)...\")\n",
    "print(f\"   Transfer learning from: {latest_checkpoint_single.name if initial_weights_multi else 'scratch'}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train(\n",
    "    env_cfg=config_multi,\n",
    "    policy_class_path=\"cogames.policy.simple.SimplePolicy\",\n",
    "    device=torch.device(\"cpu\"),\n",
    "    initial_weights_path=initial_weights_multi,  # Transfer from single-agent!\n",
    "    num_steps=200_000,\n",
    "    checkpoints_path=checkpoint_dir_multi,\n",
    "    seed=42,\n",
    "    batch_size=512,\n",
    "    minibatch_size=512,\n",
    "    vector_num_envs=4,\n",
    "    vector_num_workers=1,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Multi-agent training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef19ac1",
   "metadata": {},
   "source": [
    "### 10. Evaluate Multi-Agent Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-agent checkpoint\n",
    "checkpoint_files_multi = sorted((checkpoint_dir_multi / \"cogames.cogs_vs_clips\").glob(\"*.pt\"))\n",
    "latest_checkpoint_multi = checkpoint_files_multi[-1]\n",
    "print(f\"ðŸ“‚ Loading multi-agent checkpoint: {latest_checkpoint_multi.name}\")\n",
    "\n",
    "dummy_env_multi = MettaGridEnv(env_cfg=config_multi)\n",
    "trained_policy_multi = SimplePolicy(dummy_env_multi, device)\n",
    "trained_policy_multi.load_policy_data(str(latest_checkpoint_multi))\n",
    "\n",
    "print(\"âœ… Multi-agent policy loaded!\")\n",
    "\n",
    "# Evaluate (this will evaluate agent 0, but all agents use same policy)\n",
    "print(\"\\nðŸ“Š Evaluating multi-agent policy...\")\n",
    "metrics_multi = evaluate_policy(\n",
    "    config=config_multi,\n",
    "    policy=trained_policy_multi,\n",
    "    num_episodes=100,\n",
    "    max_steps=400,\n",
    "    seed=42\n",
    ")\n",
    "print(f\"âœ… Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70426027",
   "metadata": {},
   "source": [
    "### 11. Compare Single vs Multi-Agent Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebd576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs multi-agent\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Single-agent\n",
    "axes[0].plot(metrics_single['episode_returns'], alpha=0.3, color='blue')\n",
    "axes[0].plot(smooth_curve(metrics_single['episode_returns'], 20), \n",
    "             color='blue', linewidth=2, label='Single Agent')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Return')\n",
    "axes[0].set_title('Single-Agent Performance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Multi-agent (note: this is per-agent performance)\n",
    "axes[1].plot(metrics_multi['episode_returns'], alpha=0.3, color='red')\n",
    "axes[1].plot(smooth_curve(metrics_multi['episode_returns'], 20),\n",
    "             color='red', linewidth=2, label='Multi-Agent (per agent)')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Return')\n",
    "axes[1].set_title('Multi-Agent Performance (Individual Agent)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nðŸ“Š Performance Comparison:\")\n",
    "print(f\"   Single-agent avg return: {np.mean(metrics_single['episode_returns'][-50:]):.2f}\")\n",
    "print(f\"   Multi-agent avg return (per agent): {np.mean(metrics_multi['episode_returns'][-50:]):.2f}\")\n",
    "print(f\"   Multi-agent TEAM return (4 agents): {np.mean(metrics_multi['episode_returns'][-50:]) * 4:.2f}\")\n",
    "print()\n",
    "print(\"ðŸ’¡ Insights:\")\n",
    "print(\"   â€¢ Multi-agent per-agent return may be lower (resource competition)\")\n",
    "print(\"   â€¢ But TEAM return should be higher (4x agents working)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff880a6",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've completed all three CoGames tutorials and trained agents on progressively complex tasks.\n",
    "\n",
    "### Journey Recap\n",
    "\n",
    "```\n",
    "Tutorial 1: Given hearts â†’ Deposit\n",
    "             â†“ Added crafting\n",
    "Tutorial 2: Given resources â†’ Craft â†’ Deposit\n",
    "             â†“ Added foraging\n",
    "Tutorial 3: Start empty â†’ Forage â†’ Craft â†’ Deposit\n",
    "             â†“ Scaled to multi-agent\n",
    "Multi-Agent: 4 agents coordinating implicitly\n",
    "```\n",
    "\n",
    "### Core Concepts Mastered\n",
    "\n",
    "1. **Navigation**: Agents learned spatial reasoning\n",
    "2. **Crafting**: Recipe-based resource transformation\n",
    "3. **Foraging**: Resource extraction from environment\n",
    "4. **Multi-step Planning**: Chaining 3+ actions together\n",
    "5. **Transfer Learning**: Reusing knowledge across tasks\n",
    "6. **Multi-Agent Coordination**: Implicit cooperation through shared training\n",
    "\n",
    "### Architectural Insights\n",
    "\n",
    "- **Simple feedforward networks** can learn complex behaviors\n",
    "- **Sparse rewards** work with sufficient exploration\n",
    "- **Transfer learning** dramatically speeds up training\n",
    "- **Multi-agent** systems exhibit emergent coordination\n",
    "\n",
    "### What You Can Do Next\n",
    "\n",
    "1. **Experiment with hyperparameters**\n",
    "   - Try different learning rates\n",
    "   - Adjust network sizes\n",
    "   - Change reward structures\n",
    "\n",
    "2. **Try LSTM policy**\n",
    "   - Handles partial observability better\n",
    "   - Can remember past states\n",
    "   - See `cogames.policy.lstm.LSTMPolicy`\n",
    "\n",
    "3. **Custom scenarios**\n",
    "   - Add more resources\n",
    "   - Create custom maps\n",
    "   - Design new recipes\n",
    "\n",
    "4. **Scale further**\n",
    "   - Try 8, 16, or more agents\n",
    "   - Observe emergent specialization\n",
    "   - Analyze coordination patterns\n",
    "\n",
    "5. **Compete**\n",
    "   - Submit to CoGames competition\n",
    "   - Compare against other policies\n",
    "   - Optimize for speed/efficiency\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¾ Checkpoints Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š TUTORIAL SERIES COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Checkpoints saved:\")\n",
    "print(f\"  Tutorial 1: ./checkpoints/cogames.cogs_vs_clips/\")\n",
    "print(f\"  Tutorial 2: ./checkpoints_stage2/cogames.cogs_vs_clips/\")\n",
    "print(f\"  Tutorial 3 (single): {latest_checkpoint_single}\")\n",
    "print(f\"  Tutorial 3 (multi):  {latest_checkpoint_multi}\")\n",
    "print()\n",
    "print(\"Final Results:\")\n",
    "print(f\"  Tutorial 1 (deposit only):     ~3.0 hearts\")\n",
    "print(f\"  Tutorial 2 (craft+deposit):    ~{np.mean(metrics_single['episode_returns'][-20:]):.1f} hearts (single)\")  \n",
    "print(f\"  Tutorial 3 (full cycle):       ~{np.mean(metrics_single['episode_returns'][-20:]):.1f} hearts (single)\")\n",
    "print(f\"  Tutorial 3 (multi-agent):      ~{np.mean(metrics_multi['episode_returns'][-20:]) * 4:.1f} hearts (team of 4)\")\n",
    "print()\n",
    "print(\"ðŸŽ‰ You're now ready to:\")\n",
    "print(\"   â€¢ Design custom scenarios\")\n",
    "print(\"   â€¢ Participate in CoGames competitions\")\n",
    "print(\"   â€¢ Research multi-agent RL algorithms\")\n",
    "print(\"   â€¢ Build on these tutorials for your own projects\")\n",
    "print()\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
