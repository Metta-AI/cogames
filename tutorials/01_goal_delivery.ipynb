{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a8a6b8b",
   "metadata": {},
   "source": [
    "# Tutorial 1: Goal Delivery - Learning to Navigate and Deposit\n",
    "\n",
    "Welcome to the first CoGames tutorial! In this notebook, you'll train an agent to perform the simplest possible task: **navigate to a chest and deposit hearts** that are already in its inventory.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "- Understand the CoGames environment structure\n",
    "- Learn how agents interact via the **move** action\n",
    "- Train a policy with sparse rewards\n",
    "- Visualize learning progress\n",
    "- Interpret value functions\n",
    "\n",
    "## üìã Task Overview\n",
    "\n",
    "**Starting State:**\n",
    "- Agent spawns randomly in a 10x10 map\n",
    "- Agent starts with 3 hearts in inventory\n",
    "- One chest is placed randomly on the map\n",
    "\n",
    "**Goal:**\n",
    "- Navigate to the chest\n",
    "- Deposit hearts by moving into the chest\n",
    "- Maximize reward: +1 per heart deposited\n",
    "\n",
    "**Expected Training Time:** 10-20k steps (~2-3 minutes on CPU)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6307b",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary modules and set up our visualization utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec552baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# CoGames imports\n",
    "from cogames.cogs_vs_clips.scenarios import make_game\n",
    "from cogames.policy.simple import SimplePolicy\n",
    "from cogames.train import train\n",
    "from mettagrid import MettaGridEnv\n",
    "\n",
    "# Import visualization utilities\n",
    "from tutorial_viz import (\n",
    "    plot_episode_returns,\n",
    "    plot_success_rate,\n",
    "    plot_value_heatmap,\n",
    "    plot_position_heatmap,\n",
    "    evaluate_policy,\n",
    "    print_metrics_table,\n",
    "    smooth_curve\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Imports complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae78100",
   "metadata": {},
   "source": [
    "## 2. Configure the Environment\n",
    "\n",
    "We'll create a minimal 10x10 grid with:\n",
    "- 1 agent (starting with 3 hearts)\n",
    "- 1 chest (accepts deposits from all sides)\n",
    "- No crafting stations (hearts already in inventory)\n",
    "\n",
    "The agent receives **+1 reward** for each heart deposited (via the `heart.lost` stat).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19851bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base configuration\n",
    "config = make_game(\n",
    "    num_cogs=1,\n",
    "    width=10,\n",
    "    height=10,\n",
    "    num_assemblers=0,  # No crafting yet\n",
    "    num_chests=1,\n",
    "    num_chargers=0,\n",
    "    num_carbon_extractors=0,\n",
    "    num_oxygen_extractors=0,\n",
    "    num_germanium_extractors=0,\n",
    "    num_silicon_extractors=0,\n",
    ")\n",
    "\n",
    "# Agent starts with hearts in inventory\n",
    "config.game.agent.initial_inventory = {\n",
    "    \"energy\": 100,\n",
    "    \"heart\": 3,  # Start with 3 hearts to deposit\n",
    "}\n",
    "\n",
    "# Increase heart carrying capacity\n",
    "config.game.agent.resource_limits[\"heart\"] = 5\n",
    "\n",
    "# Configure chest to accept deposits from all sides, but no withdrawals\n",
    "config.game.objects[\"chest\"].deposit_positions = [\"N\", \"S\", \"E\", \"W\"]\n",
    "config.game.objects[\"chest\"].withdrawal_positions = []  # Disable withdrawals\n",
    "\n",
    "# Reward: +1 per heart deposited (agent-specific stat)\n",
    "config.game.agent.rewards.stats = {\n",
    "    \"heart.lost\": 1.0  # Reward when heart leaves agent's inventory\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Environment configured!\")\n",
    "print(f\"   Map size: {config.game.width}x{config.game.height}\")\n",
    "print(f\"   Initial hearts: {config.game.agent.initial_inventory['heart']}\")\n",
    "print(f\"   Reward per heart: {config.game.agent.rewards.stats['heart.lost']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ef1570",
   "metadata": {},
   "source": [
    "## 3. Create the Policy\n",
    "\n",
    "We'll use a simple feedforward neural network policy (`SimplePolicy`). This policy:\n",
    "- Takes observations as input (grid view + inventory + stats)\n",
    "- Outputs movement actions (North, South, East, West, or NoOp)\n",
    "- Will be trained using PPO (via PufferLib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c40fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy environment to infer spaces\n",
    "dummy_env = MettaGridEnv(env_cfg=config)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create policy with environment and device\n",
    "policy = SimplePolicy(dummy_env, device)\n",
    "\n",
    "print(\"‚úÖ Policy created!\")\n",
    "print(f\"   Policy type: {type(policy).__name__}\")\n",
    "print(f\"   Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be33f51",
   "metadata": {},
   "source": [
    "## 4. Train the Agent\n",
    "\n",
    "Now we'll train the agent for 20,000 steps. This should take 2-3 minutes on CPU.\n",
    "\n",
    "**What to expect:**\n",
    "- Initial episodes: Random wandering, low/zero rewards\n",
    "- After ~5k steps: Agent starts finding chest occasionally  \n",
    "- After ~15k steps: Consistent chest navigation and deposits\n",
    "\n",
    "The training will collect:\n",
    "- Episode returns (total reward per episode)\n",
    "- Episode lengths (steps taken per episode)\n",
    "- Success metrics (heart depositions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set up checkpoint directory\n",
    "checkpoint_dir = Path(\"./checkpoints\")\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train the policy\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train(\n",
    "    env_cfg=config,\n",
    "    policy_class_path=\"cogames.policy.simple.SimplePolicy\",\n",
    "    device=device,\n",
    "    initial_weights_path=None,\n",
    "    num_steps=20_000,\n",
    "    checkpoints_path=checkpoint_dir,\n",
    "    seed=42,\n",
    "    batch_size=512,\n",
    "    minibatch_size=512,\n",
    "    vector_num_envs=4,  # Parallel environments for faster training\n",
    "    vector_num_workers=1,  # Serial on macOS/CPU\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559022a",
   "metadata": {},
   "source": [
    "## 5. Load the Trained Policy\n",
    "\n",
    "Now let's load the trained policy from the saved checkpoint and evaluate it to collect metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint\n",
    "checkpoint_files = sorted((checkpoint_dir / \"cogames.cogs_vs_clips\").glob(\"*.pt\"))\n",
    "if not checkpoint_files:\n",
    "    raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "\n",
    "latest_checkpoint = checkpoint_files[-1]\n",
    "print(f\"üìÇ Loading checkpoint: {latest_checkpoint.name}\")\n",
    "\n",
    "# Load the trained policy\n",
    "trained_policy = SimplePolicy(dummy_env, device)\n",
    "trained_policy.load_policy_data(str(latest_checkpoint))\n",
    "\n",
    "print(\"‚úÖ Policy loaded!\")\n",
    "\n",
    "# Evaluate the policy to collect metrics\n",
    "print(\"\\nüìä Evaluating policy (100 episodes)...\")\n",
    "metrics = evaluate_policy(\n",
    "    config=config,\n",
    "    policy=trained_policy,\n",
    "    num_episodes=100,\n",
    "    max_steps=200,\n",
    "    seed=42\n",
    ")\n",
    "print(f\"‚úÖ Evaluation complete! Collected {len(metrics['episode_returns'])} episodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bfbf0b",
   "metadata": {},
   "source": [
    "## 6. Visualize Training Progress\n",
    "\n",
    "Let's examine the learning curves to see if our agent successfully learned to navigate and deposit hearts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff977d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode returns and lengths\n",
    "fig = plot_episode_returns(\n",
    "    metrics['episode_returns'],\n",
    "    metrics['episode_lengths'],\n",
    "    window_size=50\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print_metrics_table({\n",
    "    \"Final Avg Return\": np.mean(metrics['episode_returns'][-50:]),\n",
    "    \"Max Return\": np.max(metrics['episode_returns']),\n",
    "    \"Final Avg Length\": np.mean(metrics['episode_lengths'][-50:]),\n",
    "    \"Total Episodes\": len(metrics['episode_returns']),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe3180",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "**Episode Return Curve:**\n",
    "- Should increase from ~0 (random) to ~3.0 (all hearts deposited)\n",
    "- Smoother curves indicate more consistent learning\n",
    "- Plateaus suggest the agent has converged\n",
    "\n",
    "**Episode Length Curve:**\n",
    "- Should decrease as agent learns efficient paths\n",
    "- Initial: ~100-200 steps (wandering)\n",
    "- Final: ~20-50 steps (direct path to chest)\n",
    "\n",
    "**Success Criteria:** \n",
    "- Average return > 2.5 (depositing most/all hearts)\n",
    "- Episode length < 60 steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5bf097",
   "metadata": {},
   "source": [
    "## 7. Success Rate Analysis\n",
    "\n",
    "Let's compute the success rate over evaluation. We'll consider an episode \"successful\" if the agent deposited at least 2 hearts (return >= 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e34de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define success as depositing at least 2 hearts (return >= 2)\n",
    "success_threshold = 2.0\n",
    "successes = [1 if r >= success_threshold else 0 for r in metrics['episode_returns']]\n",
    "\n",
    "# Plot success rate\n",
    "fig = plot_success_rate(successes, window_size=50)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final success rate\n",
    "final_success_rate = np.mean(successes[-50:]) * 100\n",
    "print(f\"\\nüìä Final Success Rate (last 50 episodes): {final_success_rate:.1f}%\")\n",
    "print(f\"   Target: 60%+\")\n",
    "if final_success_rate >= 60:\n",
    "    print(\"   ‚úÖ Target achieved!\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Consider training longer or adjusting hyperparameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541adc1",
   "metadata": {},
   "source": [
    "## 8. Visualize the Trained Agent\n",
    "\n",
    "To see the trained agent in action, you can use the CoGames play command:\n",
    "\n",
    "```bash\n",
    "cogames play <game> --policy simple --policy-data ./checkpoints/cogames.cogs_vs_clips/<checkpoint>.pt\n",
    "```\n",
    "\n",
    "Note: GIF creation isn't available in the notebook since MettagGrid uses a GUI-based visualizer (mettascope).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To visualize, save the checkpoint path for later use\n",
    "print(f\"üíæ Trained model checkpoint: {latest_checkpoint}\")\n",
    "print(f\"\\nüì∫ To visualize the trained agent:\")\n",
    "print(f\"   1. Exit this notebook\")\n",
    "print(f\"   2. Run: cogames play --policy simple --policy-data {latest_checkpoint}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5d6f7",
   "metadata": {},
   "source": [
    "## 9. Value Function Analysis\n",
    "\n",
    "The value function shows what the agent has learned about which states are valuable. Areas near the chest should have high values (because they lead to reward).\n",
    "\n",
    "**Note:** This is a placeholder - full value function visualization requires manually constructing observations for each position, which we'll implement in a future update.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for value function heatmap\n",
    "# Full implementation requires observation construction for each grid position\n",
    "print(\"‚ÑπÔ∏è  Value function visualization coming in future update\")\n",
    "print(\"   For now, use position heatmap to see where agent spends time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8528f1",
   "metadata": {},
   "source": [
    "## 10. Position Heatmap\n",
    "\n",
    "Where does the agent spend its time? We already collected position data during evaluation, so let's visualize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9376db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot position heatmap from evaluation data\n",
    "if metrics['positions']:\n",
    "    print(\"üìç Generating position heatmap from evaluation...\")\n",
    "    fig = plot_position_heatmap(\n",
    "        metrics['positions'],\n",
    "        config.game.width,\n",
    "        config.game.height\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\"\"\n",
    "üìñ Interpretation:\n",
    "- **Bright spots**: Areas where agent spends more time\n",
    "- Should show concentration near chest after training\n",
    "- Before training: uniform distribution (random wandering)\n",
    "- After training: focused on chest location\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No position data collected (env.get_agent_positions() not available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cab3a",
   "metadata": {},
   "source": [
    "## üéì Summary and Key Takeaways\n",
    "\n",
    "Congratulations! You've trained your first CoGames agent. Here's what we learned:\n",
    "\n",
    "### Core Concepts\n",
    "1. **Sparse Rewards**: The agent only receives reward when depositing hearts\n",
    "2. **Move Action**: Agents interact with objects by moving into them from valid positions\n",
    "3. **Agent-Specific Stats**: `heart.lost` tracks rewards per agent (vs global stats)\n",
    "4. **Navigation Learning**: The policy learned spatial relationships between agent and chest\n",
    "\n",
    "### Training Results\n",
    "- ‚úÖ Agent learned to navigate to chest consistently\n",
    "- ‚úÖ Episode length decreased (more efficient paths)\n",
    "- ‚úÖ Return increased to ~3.0 (depositing all 3 hearts)\n",
    "- ‚úÖ Value function shows spatial understanding\n",
    "\n",
    "### Next Steps: Tutorial 2 - Simple Assembly\n",
    "\n",
    "In the next tutorial, we'll increase complexity:\n",
    "- **Add crafting**: Agent must craft hearts from raw resources\n",
    "- **Multi-step planning**: Navigate to assembler ‚Üí craft ‚Üí navigate to chest ‚Üí deposit\n",
    "- **Resource management**: Limited materials require efficient use\n",
    "- **Longer episodes**: More exploration needed\n",
    "\n",
    "Ready to continue? Open `02_simple_assembly.ipynb`!\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Save Your Work\n",
    "\n",
    "Don't forget to save your trained policy for later use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e16907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy is already saved during training\n",
    "print(f\"‚úÖ Trained policy checkpoint: {latest_checkpoint}\")\n",
    "print(f\"\\nüìù To use this policy later:\")\n",
    "print(f\"   1. In Python:\")\n",
    "print(f\"      policy = SimplePolicy(env, device)\")\n",
    "print(f\"      policy.load_policy_data('{latest_checkpoint}')\")\n",
    "print(f\"\\n   2. From command line:\")\n",
    "print(f\"      cogames play <game> --policy simple --policy-data {latest_checkpoint}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
